<script lang="ts">
  import Algorithm from "./sections/Algorithm.svelte";
  import RenderWhenVisible from "$lib/components/RenderWhenVisible.svelte";
  import Intro from "./sections/Intro.svelte";
  import Demos from "./sections/Demos.svelte";
  import { citationsData } from "./data/citations";
  import Document from "$lib/components/citations/Document.svelte";
  import Background from "./sections/Background.svelte";
  import Results from "./sections/Results.svelte";
</script>

<div id="lluminate">
  <Intro />
  <Demos />
  <Document {citationsData}>
    <!-- 
<div class="flex flex-row gap-2 items-center justify-center w-full my-4">
  <TextRadioButton
    bind:selected
    classes="text-xl gap-4"
    options={[
      { label: "Clocks", value: "clocks" },
      { label: "Image-Gen", value: "image-gen" },
      // { label: "Genetic-Algorithms", value: "genetic-algorithms" },
    ]}
  />
</div> -->

    <Algorithm />
    <Results />
    <Background />
    <div class="text_body">
      <h2>Limitations and Future work</h2>
      <p>
        <strong>Optimized strategies:</strong>
        The creative strategies were effective but arbitrarily created and there
        was no thorough evaluation of their ideal structure or content. This sets
        the stage for potentially co-evolving strategies alongside artifacts that
        adapt to specific domains. These evolved "chain of creative thought templates"
        could even be reusable in direct prompting.
      </p>
      <p>
        <strong>Quality and Constraints:</strong>
        This algorithm fails to produce valid outputs when any constraints are innately
        satisfied by the prompt, but does not have a way to repair infeasible solutions
        or any kind of minimal criteria. Potential improvements could be to apply
        a minimal criteria to the reproducing population or use the multimodal qualities
        of the models to "see" the outputs via image inputs. At the time of writing
        this, image capacities were not yet available in the o3-api, but this will
        likely change soon. Potentially - if costs lower - the entire population
        could be provided together as images allowing pressure to be applied from
        the phenotypes (images) instead of the summaries of genomes (text).
      </p>
      <p>
        <strong>Concept / Implementation barrier:</strong>
        Large outputs become costly to generate for every generation, an alternative
        approach is to only generate a shorter description of the central idea for
        the object and then do a run with a more powerful model for the final generations.
        Analogies to how most projects get evaluated at the concept phase. The main
        challenge is that many concepts generated by the LLM are impractical, diverge
        greatly when generated or are hard to articulate (such as shaders which are
        hard to describe in text). Potential future work could look to bridge this
        gap.
      </p>
    </div>
  </Document>
</div>

<!-- <style lang="postcss">
  p.label {
    @apply !w-full !text-center;
  }

  
</style> -->

<style lang="postcss">
  :global(#lluminate p) {
    @apply !my-2;
  }
  :global(#lluminate p.label) {
    @apply !w-full !text-center;
  }
  p {
    @apply py-1;
  }
  ol {
    padding-left: 2rem;
    text-align: left;
  }

  :global(#lluminate li) {
    list-style-type: decimal;
    margin-bottom: 0.5rem;
    text-align: left;
    @apply !ml-8;
  }
  strong {
    @apply font-bold;
  }

  /* h3 {
    font-size: 1.25em;
    font-weight: 500;
    margin-top: 1.5rem;
    margin-bottom: 0.75rem;
  } */
</style>
